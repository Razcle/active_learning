{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x106320d10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from hessian import *\n",
    "import operator\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=torchvision.datasets.MNIST('./', train=True, download=True,transform=torchvision.transforms.ToTensor())\n",
    "test_data=torchvision.datasets.MNIST('./', train=False, download=True,transform=torchvision.transforms.ToTensor())\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict={}\n",
    "for (data, target) in train_data:\n",
    "    if  target not in train_dict:\n",
    "        train_dict[ target]=[]\n",
    "        train_dict[ target].append(data)\n",
    "    else:\n",
    "        train_dict[ target].append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "initial_train_data=[]\n",
    "initial_train_label=[]\n",
    "for i in range(5):\n",
    "    initial_train_data.append(train_dict[i][0])\n",
    "    initial_train_label.append(i)\n",
    "    \n",
    "print(initial_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 28, 28])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "initial_train_data_tensor=torch.stack(initial_train_data)\n",
    "initial_train_label_tensor=torch.tensor(initial_train_label)\n",
    "train_x=torch.stack([data for (data, target) in train_data])\n",
    "train_label=torch.tensor([target for (data, target) in train_data])\n",
    "print(initial_train_data_tensor.size())\n",
    "print(initial_train_label_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.final_weight=torch.tensor(np.random.randn(20,10), requires_grad=True)\n",
    "        params = list(self.final_weight)\n",
    "        self.optimizer = optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "#         for name, param in self.named_parameters():\n",
    "#             if param.requires_grad:\n",
    "#                 print (name, param.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.view(-1,1,28,28)\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x= torch.matmul(x,self.final_weight)\n",
    "        return F.log_softmax(x,dim=-1)\n",
    "    \n",
    "    def predict(self,x):\n",
    "        with torch.no_grad():\n",
    "            output = torch.exp(self.forward(x))\n",
    "            pred = output.data.max(dim=1, keepdim=True)[1]\n",
    "            return pred\n",
    "    \n",
    "    def train(self,x,label):\n",
    "        train_losses = []\n",
    "        if x.size(0)>100:\n",
    "            for it in range(0,1000):\n",
    "                index=np.random.choice(x.size(0),100)\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(x[index])\n",
    "                loss = F.nll_loss(output,label[index])\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "        \n",
    "        else:    \n",
    "            for it in range(0,1000):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.forward(x)\n",
    "                loss = F.nll_loss(output,label)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                train_losses.append(loss.item())\n",
    "                print(self.final_weight[0])\n",
    "        plt.plot(train_losses)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def test(self):\n",
    "        correct=0\n",
    "        for data, target in test_loader:\n",
    "            pred = self.predict(data)\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "            correct_ratio= float(correct)/len(test_loader.dataset)\n",
    "        return correct_ratio\n",
    "        \n",
    "    def uncertainty(self,x,label):\n",
    "        output = torch.exp(self.forward(x))\n",
    "        loss = F.nll_loss(output,torch.tensor([label]))+torch.norm(network.final_weight,2)\n",
    "        jacobian_w=jacobian(output,self.final_weight)\n",
    "        hessian_w=hessian(loss,self.final_weight)+torch.eye(100)*1e-6\n",
    "        hessian_inverse=torch.inverse(hessian_w)\n",
    "        left=torch.matmul(jacobian_w,hessian_inverse)\n",
    "        pos_cov=torch.matmul(left,jacobian_w.t())\n",
    "        (sign, logdet) = np.linalg.slogdet(pos_cov.detach().numpy()) \n",
    "        entropy=5+5*np.log(2*np.pi)+0.5*logdet\n",
    "        return entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "can't optimize a non-leaf Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-392d0d6fe2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mactive_bnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mactive_bnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_train_data_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_train_label_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-0f190ae2cc79>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/rl/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     40\u001b[0m         defaults = dict(lr=lr, betas=betas, eps=eps,\n\u001b[1;32m     41\u001b[0m                         weight_decay=weight_decay, amsgrad=amsgrad)\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/rl/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_param_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/rl/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36madd_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    198\u001b[0m                                 \"but one of the params is \" + torch.typename(param))\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"can't optimize a non-leaf Tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: can't optimize a non-leaf Tensor"
     ]
    }
   ],
   "source": [
    "active_bnn = Net()\n",
    "active_bnn.train(initial_train_data_tensor,initial_train_label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnn_test_list=[]\n",
    "bnn_test_list.append(active_bnn.test())\n",
    "print(bnn_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_bnn = Net()\n",
    "active_bnn.train(initial_train_data_tensor,initial_train_label_tensor)\n",
    "bnn_test_list=[]\n",
    "bnn_test_list.append(active_bnn.test())\n",
    "print(bnn_test_list)\n",
    "current_train=initial_train_data_tensor\n",
    "current_label=initial_train_label_tensor\n",
    "for epoch in range(60):\n",
    "    index=np.random.choice(60000,10)\n",
    "    current_train=torch.cat([current_train,train_x[index]],0)\n",
    "    current_label=torch.cat([current_label,train_label[index]],0)\n",
    "    active_bnn.train(current_train,current_label)\n",
    "    ratio=active_bnn.test()\n",
    "    print('ratio',ratio)\n",
    "    bnn_test_list.append(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bnn_test_list,label='random sampling')\n",
    "plt.plot(active_list,label='active learning')\n",
    "plt.title('Sample efficiency comparison')\n",
    "plt.xlabel('number of training data (*10)') \n",
    "plt.ylabel('accuracy on test dataset')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_train=initial_train_data_tensor\n",
    "current_label=initial_train_label_tensor\n",
    "for epoch in range(600):\n",
    "    entropy_dict={}\n",
    "    for i in range(epoch*1000,(epoch+1)*1000):\n",
    "        train_epoch_x=train_x[i]\n",
    "        train_epoch_label=train_label[i]\n",
    "        entropy_dict[i]=active_bnn.uncertainty(train_x[i],train_label[i])\n",
    "    sorted_dict = sorted(entropy_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_index=[i[0] for i in sorted_dict[:10]]\n",
    "    current_train=torch.cat([current_train,train_x[sorted_index]],0)\n",
    "    current_label=torch.cat([current_label,train_label[sorted_index]],0)\n",
    "    print(current_train.size())  \n",
    "    print(current_label.size()) \n",
    "    active_bnn.train(current_train,current_label)\n",
    "    ratio=active_bnn.test()\n",
    "    print('ratio',ratio)\n",
    "    bnn_test_list.append(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_list=[]\n",
    "active_list.append(bnn_test_list[0])\n",
    "active_list.extend(bnn_test_list[3:])\n",
    "x_list=np.arange(1,62)*10\n",
    "plt.plot(x_list,active_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[0.4308, 0.6717, 0.8553, 0.6185, 0.7087, 0.7191, 0.7344, 0.7656, 0.7898, 0.831, 0.834, 0.827, 0.8636, 0.8821, 0.8717, 0.8792, 0.8892, 0.9056, 0.9103, 0.9135, 0.9116, 0.9102, 0.9268, 0.9285, 0.9287, 0.9325, 0.9387, 0.9399, 0.933, 0.93, 0.9265, 0.9374, 0.9414, 0.9447, 0.9484, 0.9461, 0.9448, 0.9361, 0.9435, 0.9456, 0.9457, 0.9497, 0.9506, 0.9527, 0.951, 0.9475, 0.9404, 0.9412, 0.9361, 0.939, 0.9435, 0.9462, 0.9504, 0.9496, 0.9539, 0.9542, 0.9573, 0.9563, 0.9587, 0.963, 0.9591, 0.9611, 0.9601]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
